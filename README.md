
# FEAT: Feedback & Evaluation via Automated Tests

**FEAT** (Feedback & Evaluation via Automated Tests) is a Java-based system that automatically generates a concise, effective test suite for Python functions, given:
1. A configuration file specifying the function signature and input domains  
2. A correct *reference* implementation  
3. A corpus of *buggy* implementations  

By comparing the outputs of candidate implementations against the reference solution on a large “base set” of tests, FEAT selects a minimal subset of test cases that still catches all known buggy behaviors. These test cases can then be used for both summative evaluation (grading) and formative feedback (guided hints).  
&nbsp;  
> Originally developed at Rice in 2016 and used to generate many of the test cases for COMP 140 and Rice-developed MOOCs. :contentReference[oaicite:0]{index=0}

---

## Table of Contents
1. [Key Features](#key-features)   
3. [Inputs & Outputs](#inputs--outputs)  
4. [Directory Structure](#directory-structure)  
5. [Installation & Build](#installation--build)  
6. [Usage Example](#usage-example)  


---

## Key Features

- **Data-Driven Test Generation**  
  - FEAT uses a corpus of known buggy implementations to train a black-box test generator :contentReference[oaicite:1]{index=1}.  
  - It outputs both an *exhaustive* base set (all possible inputs within a constrained domain) and a small number of *random* tests (for a wider domain).  

- **Automated Concise Set Selection**  
  - From the full base set, FEAT determines which buggy implementations fail which test cases.  
  - It then computes a near-minimal subset of tests that still distinguishes all buggy variants from the reference, ensuring fast evaluation in platforms like OwlTest. :contentReference[oaicite:2]{index=2}  

- **Language-Agnostic Core Logic**  
  - Although the original FEAT paper used Python, this repository re-implements the test-generation logic in Java (for Python functions).  
  - You can extend FEAT to other target languages by writing new adapters.  
 
- **Summative & Formative Feedback**  
  - The selected concise test suite can be used either to grade a student submission (summative) or to provide targeted feedback on specific failing cases (formative). :contentReference[oaicite:3]{index=3}

---


1. **Parsing Component**  
   - Reads the JSON configuration (see section below) to produce a *test generation template*. :contentReference[oaicite:4]{index=4}  
   - The template encodes function name, parameter types, and allowable input domains.

2. **Base Set Generator**  
   - **Exhaustive Tests:** Enumerates *all* possible combinations within a small, constrained domain (e.g., all pairs of integers from −10 to 10).  
   - **Random Tests:** Samples a user-specified number of random inputs from a larger domain (e.g., −150 to 150). :contentReference[oaicite:5]{index=5}  

3. **Test Harness**  
   - Runs each test case in the base set against:  
     - The *reference* (correct) implementation  
     - Each *buggy* implementation in the training corpus  
   - Records which test cases each buggy variant fails on (i.e., output mismatch with reference).

4. **Concise Set Selector**  
   - Uses a (near-)greedy algorithm to pick a minimal subset of base tests such that every buggy implementation is “caught” (i.e., fails at least one selected test).  
   - Outputs the concise suite in a form that can be consumed by an auto-grader (e.g., OwlTest). :contentReference[oaicite:6]{index=6}  

---

## Inputs & Outputs

1. **Inputs**  
   - **Configuration File** (JSON)  
     - Specifies:  
       1. `fname`: function name (e.g., `multiply`)  
       2. `types`: an array of parameter types (e.g., `["int","int"]`)  
       3. `exhaustive domain`: an array of strings denoting inclusive integer ranges (e.g., `["-10~10","-10~10"]`)  
       4. `random domain`: an array of strings for a broader range (e.g., `["-150~150","-150~150"]`)  
       5. `num random`: number of random test cases to generate (e.g., `3`)  
     - Example:
       ```json
       {
         "fname": "multiply",
         "types": ["int", "int"],
         "exhaustive domain": ["-10~10", "-10~10"],
         "random domain": ["-150~150", "-150~150"],
         "num random": 3
       }
       ```
       :contentReference[oaicite:7]{index=7}

   - **Reference Solution**: A single Python `.py` file containing a correct implementation of the function under test.  
   - **Buggy Implementations**: A directory of Python files, each containing a student-like buggy implementation of the same function signature.

2. **Outputs**  
   - `base_tests.json` (or CSV): List of all test-case tuples generated by combining exhaustive + random domains.  
   - `test_results/`: A mapping from each buggy implementation to the set of base-test indices it fails on.  
   - `concise_tests.json` (or CSV): The final, minimal test suite that catches every buggy variant.  

---

## Directory Structure

Below is a recommended layout; feel free to adapt as you see fit:



